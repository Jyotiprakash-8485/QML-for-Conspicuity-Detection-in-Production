{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the parity function\n",
        "==========================="
      ],
      "metadata": {
        "id": "GG_IN_xGlxmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example shows that a variational circuit can\n",
        "be optimized to emulate the parity function\n",
        "\n",
        "$$\\begin{aligned}\n",
        "f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y =\n",
        "\\begin{cases} 1 \\text{  if uneven number of 1's in } x \\\\ 0\n",
        "\\text{ else}. \\end{cases}\n",
        "\\end{aligned}$$\n",
        "\n",
        "We are building a ML model which will demonstrate how to encode binary inputs into the initial state of\n",
        "the variational circuit, which is simply a computational basis state\n",
        "(*basis encoding*)."
      ],
      "metadata": {
        "id": "8xjoPWpMp8er"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7IXJqmsllv3s",
        "outputId": "2e9ef0f9-4b65-4bfe-ca76-6c3b1021015f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.4.0)\n",
            "Collecting pennylane-lightning>=0.37 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.7.4)\n",
            "Downloading PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.6.12 pennylane-0.37.0 pennylane-lightning-0.37.0 rustworkx-0.15.1 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.optimize import NesterovMomentumOptimizer\n",
        "from typing import Tuple"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "a4C_Y2gNifj6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for the model and training\n",
        "config = {\n",
        "    \"num_qubits\": 4, # Number of qubits in the quantum circuit\n",
        "    \"num_layers\": 2, # Number of layers in the variational circuit\n",
        "    \"learning_rate\": 0.5, # Learning rate for the optimizer\n",
        "    \"batch_size\": 5, # Batch size for training\n",
        "    \"num_iterations\": 100, # Number of training iterations\n",
        "    \"train_data_path\": \"/content/train data.txt\", # Path to the training data file\n",
        "    \"test_data_path\": \"/content/test data.txt\" # Path to the testing data file\n",
        "}"
      ],
      "metadata": {
        "id": "nIA9z37Hl7ar"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading\n",
        "def load_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Loads data from a text file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the data file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the input features (X) and labels (Y).\n",
        "    \"\"\"\n",
        "    data = np.loadtxt(file_path, dtype=int)\n",
        "    X = np.array(data[:, :-1])\n",
        "    Y = np.array(data[:, -1])\n",
        "    Y = Y * 2 - 1  # Shifts labels from {0, 1} to {-1, 1} for binary classification\n",
        "    return X, Y\n"
      ],
      "metadata": {
        "id": "ZYmgdOGUl_0C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Quantum Circuit Layer\n",
        "\n",
        "This cell defines a function `layer` that represents a single layer of the variational quantum circuit. It applies rotation gates to each qubit and CNOT gates between pairs of qubits."
      ],
      "metadata": {
        "id": "wONdkoW21zZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum circuit definition\n",
        "def layer(layer_weights):\n",
        "    \"\"\"A single layer of the variational circuit.\"\"\"\n",
        "    for wire in range(config[\"num_qubits\"]):\n",
        "        qml.Rot(*layer_weights[wire], wires=wire) # Applies a rotation gate to each qubit\n",
        "\n",
        "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
        "        qml.CNOT(wires)"
      ],
      "metadata": {
        "id": "HJs0Bi7l1yWN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State preparation function\n",
        "def state_preparation(x):\n",
        "    \"\"\"Encodes the input data into the quantum state.\"\"\"\n",
        "    qml.BasisState(x, wires=range(config[\"num_qubits\"])) # Encodes the input as a basis state"
      ],
      "metadata": {
        "id": "v9EiGLbI2JyP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantum Device\n",
        "\n",
        "This cell initializes a quantum device dev using qml.device. The \"default.qubit\" device simulates a quantum computer, allowing us to run and test the quantum circuit without access to a physical quantum computer."
      ],
      "metadata": {
        "id": "YZfTew1E4H_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantum Node\n",
        "\n",
        "This cell defines the core quantum computation as a quantum node circuit using the @qml.qnode decorator.\n",
        "\n",
        "\n",
        "*   **Input and Weights:** The function takes the trainable weights and the input data x as parameters.\n",
        "*   **State Preparation and Layers:** It first prepares the quantum state using state_preparation(x) and then applies the layers of the variational circuit using the provided weights.\n",
        "\n",
        "\n",
        "*   **Measurement:** Finally, it measures the expectation value of the Pauli Z operator (qml.PauliZ(0)) on the first qubit. This expectation value represents the output of the quantum circuit and will be used for classification.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A-eqCMow4kjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum device initialization\n",
        "dev = qml.device(\"default.qubit\")\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, x):\n",
        "    \"\"\"The quantum circuit for the variational classifier.\"\"\"\n",
        "    state_preparation(x)\n",
        "\n",
        "    for layer_weights in weights:\n",
        "        layer(layer_weights) # Applies each layer to the quantum state\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))"
      ],
      "metadata": {
        "id": "pHSeYdp44Ght"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Variational Classifier Model\n",
        "\n",
        "This cell defines the variational_classifier function, which represents the overall model. It takes the weights, bias, and input x as parameters. It computes the output of the quantum circuit circuit(weights, x) and adds a bias term to it. This output serves as the model's prediction."
      ],
      "metadata": {
        "id": "gjV95Cc15LdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variational_classifier(weights, bias, x):\n",
        "    \"\"\"The variational classifier model.\"\"\"\n",
        "    return circuit(weights, x) + bias\n"
      ],
      "metadata": {
        "id": "idvVZidlmHo7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss and Accuracy Functions\n",
        "\n",
        "This cell defines two helper functions:\n",
        "\n",
        "*   **square_loss:** Calculates the mean squared error (MSE) loss between the true labels and the model's predictions. MSE is a common loss function for regression tasks.\n",
        "*   **accuracy:** Calculates the accuracy of the model's predictions by counting the number of correct predictions and dividing by the total number of predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Hk1SMwZ5bvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and accuracy functions\n",
        "def square_loss(labels, predictions):\n",
        "    \"\"\"Calculates the mean squared error loss.\"\"\"\n",
        "    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "    \"\"\"Calculates the accuracy of the predictions.\"\"\"\n",
        "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
        "    return acc / len(labels)\n"
      ],
      "metadata": {
        "id": "vxzVDht18nLt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cost Function\n",
        "\n",
        "This cell defines the cost function, which is the objective function that the optimizer will try to minimize during training. It takes the weights, bias, input features `X`, and true labels `Y` as parameters.\n",
        "\n",
        "**Prediction Calculation:** It first computes the predictions for all inputs using the `variational_classifier`.\n",
        "\n",
        "**Loss Calculation:** It then calculates the mean squared error loss between the true labels and the predictions."
      ],
      "metadata": {
        "id": "0VYMv44-5sS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def cost(weights, bias, X, Y):\n",
        "    \"\"\"The cost function to be minimized.\"\"\"\n",
        "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
        "    return square_loss(Y, predictions) # Returns the mean squared error loss\n"
      ],
      "metadata": {
        "id": "4Y3MrQ0imLWM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training\n",
        "\n",
        "This cell defines the train_model function that orchestrates the training process.\n",
        "\n",
        "**Initialization:** It initializes the weights randomly and the bias to zero. The `requires_grad=True` flag indicates that these parameters should be tracked for gradient computation during optimization.\n",
        "\n",
        "**Training Loop:** It iterates over a specified number of training iterations. In each iteration:\n",
        "\n",
        "\n",
        "*   It randomly selects a batch of data.\n",
        "*   It updates the weights and bias using the optimizer's `step` method, which computes gradients and adjusts the parameters to minimize the cost function.\n",
        "\n",
        "*   It calculates and prints the current cost and accuracy on the entire training set to monitor progress.\n",
        "\n",
        "**Return Trained Parameters:** After training, it returns the optimized weights and bias."
      ],
      "metadata": {
        "id": "4ZEqEI9I514a"
      }
    },
    {
      "source": [
        "# Model training\n",
        "def train_model(X, Y, optimizer, batch_size, num_iterations):\n",
        "    \"\"\"Trains the variational classifier.\"\"\"\n",
        "    np.random.seed(0)\n",
        "    weights_init = 0.01 * np.random.randn(config[\"num_layers\"], config[\"num_qubits\"], 3, requires_grad=True)\n",
        "    bias_init = np.array(0.0, requires_grad=True)\n",
        "\n",
        "    weights = weights_init\n",
        "    bias = bias_init\n",
        "\n",
        "    for it in range(num_iterations):\n",
        "        batch_index = np.random.randint(0, len(X), (batch_size,))\n",
        "        X_batch = X[batch_index] # Extracts input features for the batch\n",
        "        Y_batch = Y[batch_index] # Extracts labels for the batch\n",
        "        weights, bias = optimizer.step(cost, weights, bias, X=X_batch, Y=Y_batch) # Updates weights and bias using the optimizer\n",
        "\n",
        "        # Compute and print training progress\n",
        "        predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
        "        current_cost = cost(weights, bias, X, Y)\n",
        "        acc = accuracy(Y, predictions)\n",
        "        print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")\n",
        "\n",
        "    return weights, bias\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZmpT-KLSij7i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Execution (Training)\n",
        "\n",
        "This cell is the main execution block for training the model. It checks if the script is being run as the main program (not imported as a module). If so, it:\n",
        "\n",
        "**Loads Data:** Loads the training and testing data using the `load_data` function.\n",
        "\n",
        "**Initializes Optimizer:** Creates an instance of the Nesterov Momentum Optimizer with the specified learning rate.\n",
        "\n",
        "**Trains Model:** Calls the `train_model` function to train the variational classifier using the training data and the optimizer."
      ],
      "metadata": {
        "id": "AcUuL4Zp6Ktw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    X_train, Y_train = load_data(config[\"train_data_path\"])\n",
        "    X_test, Y_test = load_data(config[\"test_data_path\"])\n",
        "\n",
        "    opt = NesterovMomentumOptimizer(config[\"learning_rate\"])\n",
        "    weights, bias = train_model(X_train, Y_train, opt, config[\"batch_size\"], config[\"num_iterations\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEAuhFZPmWdi",
        "outputId": "8b93d9e5-bdae-498b-e1ce-9676bbc0bd10"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter:    1 | Cost: 2.3147651 | Accuracy: 0.5000000\n",
            "Iter:    2 | Cost: 1.9664866 | Accuracy: 0.5000000\n",
            "Iter:    3 | Cost: 1.9208589 | Accuracy: 0.5000000\n",
            "Iter:    4 | Cost: 2.6276126 | Accuracy: 0.5000000\n",
            "Iter:    5 | Cost: 0.9323119 | Accuracy: 0.6000000\n",
            "Iter:    6 | Cost: 1.1903549 | Accuracy: 0.5000000\n",
            "Iter:    7 | Cost: 2.0508989 | Accuracy: 0.4000000\n",
            "Iter:    8 | Cost: 1.1275531 | Accuracy: 0.6000000\n",
            "Iter:    9 | Cost: 1.1659803 | Accuracy: 0.6000000\n",
            "Iter:   10 | Cost: 1.1349618 | Accuracy: 0.6000000\n",
            "Iter:   11 | Cost: 0.9994063 | Accuracy: 0.6000000\n",
            "Iter:   12 | Cost: 1.0812559 | Accuracy: 0.6000000\n",
            "Iter:   13 | Cost: 1.2863155 | Accuracy: 0.6000000\n",
            "Iter:   15 | Cost: 1.1323724 | Accuracy: 0.6000000\n",
            "Iter:   16 | Cost: 1.3439737 | Accuracy: 0.8000000\n",
            "Iter:   17 | Cost: 2.0076168 | Accuracy: 0.6000000\n",
            "Iter:   18 | Cost: 1.2685760 | Accuracy: 0.5000000\n",
            "Iter:   19 | Cost: 1.6762475 | Accuracy: 0.5000000\n",
            "Iter:   20 | Cost: 1.1868237 | Accuracy: 0.6000000\n",
            "Iter:   21 | Cost: 1.4784687 | Accuracy: 0.6000000\n",
            "Iter:   22 | Cost: 1.4599473 | Accuracy: 0.6000000\n",
            "Iter:   23 | Cost: 0.9573269 | Accuracy: 0.6000000\n",
            "Iter:   24 | Cost: 1.1657424 | Accuracy: 0.5000000\n",
            "Iter:   25 | Cost: 1.0877087 | Accuracy: 0.4000000\n",
            "Iter:   26 | Cost: 1.1683687 | Accuracy: 0.6000000\n",
            "Iter:   27 | Cost: 2.1141689 | Accuracy: 0.6000000\n",
            "Iter:   28 | Cost: 1.0272966 | Accuracy: 0.5000000\n",
            "Iter:   29 | Cost: 0.9664085 | Accuracy: 0.5000000\n",
            "Iter:   30 | Cost: 1.1287654 | Accuracy: 0.6000000\n",
            "Iter:   31 | Cost: 1.4202360 | Accuracy: 0.4000000\n",
            "Iter:   32 | Cost: 1.1286000 | Accuracy: 0.5000000\n",
            "Iter:   33 | Cost: 1.9594333 | Accuracy: 0.4000000\n",
            "Iter:   34 | Cost: 1.2811832 | Accuracy: 0.4000000\n",
            "Iter:   35 | Cost: 0.8522775 | Accuracy: 0.7000000\n",
            "Iter:   36 | Cost: 1.4765281 | Accuracy: 0.6000000\n",
            "Iter:   37 | Cost: 0.9603287 | Accuracy: 0.6000000\n",
            "Iter:   38 | Cost: 1.6031314 | Accuracy: 0.6000000\n",
            "Iter:   39 | Cost: 1.1700888 | Accuracy: 0.4000000\n",
            "Iter:   40 | Cost: 1.7571779 | Accuracy: 0.4000000\n",
            "Iter:   41 | Cost: 1.9608116 | Accuracy: 0.6000000\n",
            "Iter:   42 | Cost: 2.0802752 | Accuracy: 0.6000000\n",
            "Iter:   43 | Cost: 1.1904884 | Accuracy: 0.3000000\n",
            "Iter:   44 | Cost: 0.9941585 | Accuracy: 0.6000000\n",
            "Iter:   45 | Cost: 1.0709609 | Accuracy: 0.5000000\n",
            "Iter:   46 | Cost: 0.9780625 | Accuracy: 0.6000000\n",
            "Iter:   47 | Cost: 1.1573709 | Accuracy: 0.6000000\n",
            "Iter:   48 | Cost: 1.0235239 | Accuracy: 0.6000000\n",
            "Iter:   49 | Cost: 1.2842469 | Accuracy: 0.5000000\n",
            "Iter:   50 | Cost: 0.8549226 | Accuracy: 0.6000000\n",
            "Iter:   51 | Cost: 0.5136787 | Accuracy: 1.0000000\n",
            "Iter:   52 | Cost: 0.2488031 | Accuracy: 1.0000000\n",
            "Iter:   53 | Cost: 0.0461277 | Accuracy: 1.0000000\n",
            "Iter:   54 | Cost: 0.0293518 | Accuracy: 1.0000000\n",
            "Iter:   55 | Cost: 0.0205454 | Accuracy: 1.0000000\n",
            "Iter:   56 | Cost: 0.0352514 | Accuracy: 1.0000000\n",
            "Iter:   57 | Cost: 0.0576767 | Accuracy: 1.0000000\n",
            "Iter:   58 | Cost: 0.0291305 | Accuracy: 1.0000000\n",
            "Iter:   59 | Cost: 0.0127137 | Accuracy: 1.0000000\n",
            "Iter:   60 | Cost: 0.0058108 | Accuracy: 1.0000000\n",
            "Iter:   61 | Cost: 0.0018002 | Accuracy: 1.0000000\n",
            "Iter:   62 | Cost: 0.0014089 | Accuracy: 1.0000000\n",
            "Iter:   63 | Cost: 0.0017489 | Accuracy: 1.0000000\n",
            "Iter:   64 | Cost: 0.0021282 | Accuracy: 1.0000000\n",
            "Iter:   65 | Cost: 0.0029876 | Accuracy: 1.0000000\n",
            "Iter:   66 | Cost: 0.0035331 | Accuracy: 1.0000000\n",
            "Iter:   67 | Cost: 0.0035540 | Accuracy: 1.0000000\n",
            "Iter:   68 | Cost: 0.0025639 | Accuracy: 1.0000000\n",
            "Iter:   69 | Cost: 0.0019459 | Accuracy: 1.0000000\n",
            "Iter:   70 | Cost: 0.0015856 | Accuracy: 1.0000000\n",
            "Iter:   71 | Cost: 0.0008439 | Accuracy: 1.0000000\n",
            "Iter:   72 | Cost: 0.0005960 | Accuracy: 1.0000000\n",
            "Iter:   73 | Cost: 0.0003122 | Accuracy: 1.0000000\n",
            "Iter:   74 | Cost: 0.0002446 | Accuracy: 1.0000000\n",
            "Iter:   75 | Cost: 0.0001745 | Accuracy: 1.0000000\n",
            "Iter:   76 | Cost: 0.0001215 | Accuracy: 1.0000000\n",
            "Iter:   77 | Cost: 0.0001141 | Accuracy: 1.0000000\n",
            "Iter:   78 | Cost: 0.0001538 | Accuracy: 1.0000000\n",
            "Iter:   79 | Cost: 0.0001871 | Accuracy: 1.0000000\n",
            "Iter:   80 | Cost: 0.0001330 | Accuracy: 1.0000000\n",
            "Iter:   81 | Cost: 0.0001380 | Accuracy: 1.0000000\n",
            "Iter:   82 | Cost: 0.0001336 | Accuracy: 1.0000000\n",
            "Iter:   83 | Cost: 0.0001483 | Accuracy: 1.0000000\n",
            "Iter:   84 | Cost: 0.0001234 | Accuracy: 1.0000000\n",
            "Iter:   85 | Cost: 0.0001359 | Accuracy: 1.0000000\n",
            "Iter:   86 | Cost: 0.0001268 | Accuracy: 1.0000000\n",
            "Iter:   87 | Cost: 0.0002270 | Accuracy: 1.0000000\n",
            "Iter:   88 | Cost: 0.0000865 | Accuracy: 1.0000000\n",
            "Iter:   89 | Cost: 0.0000774 | Accuracy: 1.0000000\n",
            "Iter:   90 | Cost: 0.0000759 | Accuracy: 1.0000000\n",
            "Iter:   91 | Cost: 0.0000607 | Accuracy: 1.0000000\n",
            "Iter:   92 | Cost: 0.0000523 | Accuracy: 1.0000000\n",
            "Iter:   93 | Cost: 0.0000536 | Accuracy: 1.0000000\n",
            "Iter:   94 | Cost: 0.0000444 | Accuracy: 1.0000000\n",
            "Iter:   95 | Cost: 0.0000384 | Accuracy: 1.0000000\n",
            "Iter:   96 | Cost: 0.0000497 | Accuracy: 1.0000000\n",
            "Iter:   97 | Cost: 0.0000263 | Accuracy: 1.0000000\n",
            "Iter:   98 | Cost: 0.0000229 | Accuracy: 1.0000000\n",
            "Iter:   99 | Cost: 0.0000339 | Accuracy: 1.0000000\n",
            "Iter:  100 | Cost: 0.0000174 | Accuracy: 1.0000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation\n",
        "\n",
        "This cell defines the evaluate_model function, which evaluates the performance of the trained model on the test set.\n",
        "\n",
        "**Prediction Calculation:** It computes predictions for all test inputs using the trained weights and bias.\n",
        "**Individual Predictions:** It iterates through the test inputs, true labels, and predictions, printing each individual prediction.\n",
        "**Accuracy Calculation:** It calculates and prints the overall accuracy of the model on the test set."
      ],
      "metadata": {
        "id": "h4GBvUj16Whz"
      }
    },
    {
      "source": [
        "# Model evaluation\n",
        "def evaluate_model(weights, bias, X, Y):\n",
        "    \"\"\"Evaluates the trained model on the test set.\"\"\"\n",
        "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
        "\n",
        "    for x, y, p in zip(X, Y, predictions):\n",
        "        print(f\"x = {x}, y = {y}, pred={p}\")\n",
        "\n",
        "    acc = accuracy(Y, predictions)\n",
        "    print(\"Accuracy on unseen data:\", acc)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "D1QhjHMKo7u_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Execution (Evaluation)\n",
        "\n",
        "This cell is the main execution block for evaluating the trained model. It checks if the script is being run as the main program. If so, it:\n",
        "\n",
        "\n",
        "*   **Loads Testing Data:** Loads the testing data using the `load_data` function.\n",
        "*   **Evaluates Model:** Calls the `evaluate_model` function to evaluate the trained model on the testing data, providing insights into the model's generalization performance on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dvi4qB4G6oFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution (evaluation part)\n",
        "if __name__ == \"__main__\":\n",
        "    X_test, Y_test = load_data(config[\"test_data_path\"])\n",
        "    evaluate_model(weights, bias, X_test, Y_test)"
      ],
      "metadata": {
        "id": "YHSnXiSa6m1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34e3549-51bf-4004-a1da-8409ed173c7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [0 0 0 0], y = -1, pred=-1.0\n",
            "x = [0 0 1 1], y = -1, pred=-1.0\n",
            "x = [1 0 1 0], y = -1, pred=-1.0\n",
            "x = [1 1 1 0], y = 1, pred=1.0\n",
            "x = [1 1 0 0], y = -1, pred=-1.0\n",
            "x = [1 1 0 1], y = 1, pred=1.0\n",
            "Accuracy on unseen data: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This comprehensive explanation covers the purpose and functionality of each step in the provided code, offering a detailed understanding of how the variational quantum classifier is constructed, trained, and evaluated."
      ],
      "metadata": {
        "id": "4mOlyA9y6yhy"
      }
    }
  ]
}